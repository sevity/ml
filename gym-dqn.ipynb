{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gym.. pytorch로 DQN돌려보기\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "위 링크보고 따라해본거\n",
    "\n",
    "인스톨정보..\n",
    "pytorch\n",
    "\n",
    "https://pytorch.org/ 여기보고 \n",
    "\n",
    "conda install pytorch torchvision -c pytorch\n",
    "\n",
    "위처럼 명령어 실행함\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torchvision\n",
    "\n",
    "https://anaconda.org/soumith/torchvision 여기보고\n",
    "\n",
    "conda install -c soumith torchvision \n",
    "\n",
    "conda install -c soumith/label/pytorch torchvision \n",
    "\n",
    "\n",
    "위처럼 명령어 차례대로 실행함\n",
    "\n",
    "전반적인소감 : 별로 어려울거 없고 평이하게 잘 작성되어 있고 딥마인드의 핵심개념인 리플레이메모리랑, 스테이셔너리가 잘 구현돼 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CartPole task is designed so that the inputs to the agent are 4 real values representing the environment state (position, velocity, etc.). \n",
    "\n",
    "인풋은 이런거고.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, neural networks can solve the task purely by looking at the scene, so we’ll use a patch of the screen centered on the cart as an input. Because of this, our results aren’t directly comparable to the ones from the official leaderboard - our task is much harder. \n",
    "\n",
    "오호.. 이미지로 하는거구만 ㅇㅋ\n",
    "\n",
    "아래 군데군데 주석달아놨으니까 나중에 참고하시라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# 최근 state전이 pair 만개를 저장해두고 그야말로 랜덤하게 128개 배치뽑아서 트레이닝\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to check that below class is same thing as deepmind version\n",
    "# 음.. 이게.. 왼쪽 오른쪽 각 action에 대한 확률이 동시에 나오는 구조인거 같다.\n",
    "# [0.2, 0.3] 이렇게 나오면 오른쪽으로 갈 확률이 더 높은식.. max해서 쓰는식..\n",
    "# 암튼 이건 value gradient 방식이니까 policy gradient방식과 다르게 max해서 한쪽으로만 가는거다. \n",
    "# policy gradient방식에서는 확률적으로 분기해서 가는 방식\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 2)\n",
    "\n",
    "    # 실제 x에는 [128,3,40,80] 이런 shape이 들어오는데 앞에 128개면 128개를 병렬로 수행하면서 128개 output을 뱉어내주고 이런구조인거 같다.\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 아래 get_screen() 이함수는 다른 때에도 활용도가 있을듯!!\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "# This is based on the code from gym.\n",
    "screen_width = 600\n",
    "\n",
    "\n",
    "def get_cart_location():\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "\n",
    "# pyplot에서 rgb버퍼를 가져오는 코드인데.. 재밌는건 cart위치 중심으로 crop을 한다. 이것 때문에 stackoverflow에도 말이 있던데\n",
    "# 이 crop을 하지 않으면 잘 학습이 안된다고 한다.. \n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose(\n",
    "        (2, 0, 1))  # transpose into torch order (CHW)\n",
    "    # Strip off the top and bottom of the screen\n",
    "    screen = screen[:, 160:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescare, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "# 아래는 내가 rgb버퍼가 있을때 pyplot에다가 그릴수 있는 기능이다. 알아두면 좋겠지??\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "# 입실론 그리디가 적용되어 있고 입실론은 시간이 갈수록 줄어드는 구조인듯하다(EPS_DECAY)\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "# 요거는 에피소드가 진행됨에 따라 몇스텝이나 살아남았는지를 y축에 두고 누적그래프를 그려주는 코드이다.\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래코드가 학습에 있어서 핵심인데..\n",
    "# Q(s_t, a)랑 max Q(s_t+1, a)를 둘다 구해서 로스구하고 역전파하는 방식인데..\n",
    "# stationary를 위해서 왼쪽거는 policy network, 오른쪽꺼는 target network을 사용하고\n",
    "# target network은 10번 에피소드에 한번씩만 policy network과 동기화 해주는 구조 (어려운 개념은 없다.)\n",
    "# 아래에서 배치사이즈 만큼 한번에 step돌리는거 같은데 그 구체적인 방법에 대해서 자세히 보지는 않았다. 나중에는 한번쯤은 봐야할듯\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # 봉이 넘어져서 에피소드가 끝난경우는 next_state에 None을 넣어주는데 그걸 체크해주는 부분\n",
    "    # 여러 에피소드가 떡이 되어 있다 보니 그걸 구분하는 게 필요하다고 이해하면 좋을듯 하다.\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    \n",
    "    # 이건 None인건 아예 빼버려서 위에변수랑 숫자가 다르다. 위에건 무조건 BATCH_SIZE인 128개, 아래건 121개도 나오고 가변적\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    # policy_net만 테우면 [왼쪽확률,오른쪽확률]이거 128개가 나오는데..(shape으로는 [128,2])\n",
    "    # 실제 행해진 동작인 action_batch(이건 [128,1]이다 one hot encoding이 아니라서 그럼)에 맞는거만 모아서 [128,1]로 바꿈\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    # 아래건 아직 좀 헷갈리긴 한데..\n",
    "    # 위에서 에피소드 끝 표시자인 None을 빼고 가변적으로 뽑은 예를들어 121개에 대해서 타겟넷 forward 태우고\n",
    "    # max()함수가 애매한데 내생각엔 왼쪽 오른쪽 2개중에 큰값으로 하나만 해서 다시 121개 만드는거 같다.\n",
    "    # 액션이 왼쪽, 오른쪽 두개니까 두개해보고 큰걸 고르면 되는식인거겠지\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        # 요개 내부 파라미터를 -1에서 1사이로 매번 조정해주는 기능같은데.. 왜 쓰는지 모르겠다.\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 아래가 메인루프라고 할 수 있겠다.\n",
    "# 이게 돌려보니까.. 입실론 그리디에서 랜덤으로 빠질때는 에피소드는 하나 소모되지만 실제 네트웍학습은 스킵되고\n",
    "# 리플레이 메모리 사이즈가 배치사이즈보다 작을때도 스킵되는등 초반에는 허수가 있다.\n",
    "# 실제로는 50은 택도없고 꽤나 큰수가 돼야 수렴되기 시작하는걸로 보인다.\n",
    "\n",
    "# 그리고 reward구조를 보면 step을 돌렸을때 살아남기만 하면 1.0을 주고\n",
    "# pole의 기울기가 너무 커지면 done이 True가 되면서 에피소드가 끝나는 구조이다.\n",
    "# 마지막에 done이 True일때 reward가 0으로 반영되는지는 확인안해봤는데 나중에 확인해보자. 0이라면 터미널 패널티가 있는 셈일듯 \n",
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
