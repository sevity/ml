{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gym.. pytorch로 DQN돌려보기\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "위 링크보고 따라해본거\n",
    "\n",
    "인스톨정보..\n",
    "pytorch\n",
    "\n",
    "https://pytorch.org/ 여기보고 \n",
    "\n",
    "conda install pytorch torchvision -c pytorch\n",
    "\n",
    "위처럼 명령어 실행함\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torchvision\n",
    "\n",
    "https://anaconda.org/soumith/torchvision 여기보고\n",
    "\n",
    "conda install -c soumith torchvision \n",
    "\n",
    "conda install -c soumith/label/pytorch torchvision \n",
    "\n",
    "\n",
    "위처럼 명령어 차례대로 실행함\n",
    "\n",
    "전반적인소감 : 별로 어려울거 없고 평이하게 잘 작성되어 있고 딥마인드의 핵심개념인 리플레이메모리랑, 스테이셔너리가 잘 구현돼 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CartPole task is designed so that the inputs to the agent are 4 real values representing the environment state (position, velocity, etc.). \n",
    "\n",
    "인풋은 이런거고.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, neural networks can solve the task purely by looking at the scene, so we’ll use a patch of the screen centered on the cart as an input. Because of this, our results aren’t directly comparable to the ones from the official leaderboard - our task is much harder. \n",
    "\n",
    "오호.. 이미지로 하는거구만 ㅇㅋ\n",
    "\n",
    "아래 군데군데 주석달아놨으니까 나중에 참고하시라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# 최근 state전이 pair 만개를 저장해두고 그야말로 랜덤하게 128개 배치뽑아서 트레이닝\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to check that below class is same thing as deepmind version\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 아래 get_screen() 이함수는 다른 때에도 활용도가 있을듯!!\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "# This is based on the code from gym.\n",
    "screen_width = 600\n",
    "\n",
    "\n",
    "def get_cart_location():\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "\n",
    "# pyplot에서 rgb버퍼를 가져오는 코드인데.. 재밌는건 cart위치 중심으로 crop을 한다. 이것 때문에 stackoverflow에도 말이 있던데\n",
    "# 이 crop을 하지 않으면 잘 학습이 안된다고 한다.. \n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose(\n",
    "        (2, 0, 1))  # transpose into torch order (CHW)\n",
    "    # Strip off the top and bottom of the screen\n",
    "    screen = screen[:, 160:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescare, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "# 아래는 내가 rgb버퍼가 있을때 pyplot에다가 그릴수 있는 기능이다. 알아두면 좋겠지??\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "# 입실론 그리디가 적용되어 있고 입실론은 시간이 갈수록 줄어드는 구조인듯하다(EPS_DECAY)\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "# 요거는 에피소드가 진행됨에 따라 몇스텝이나 살아남았는지를 y축에 두고 누적그래프를 그려주는 코드이다.\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래코드가 학습에 있어서 핵심인데..\n",
    "# Q(s_t, a)랑 max Q(s_t+1, a)를 둘다 구해서 로스구하고 역전파하는 방식인데..\n",
    "# stationary를 위해서 왼쪽거는 policy network, 오른쪽꺼는 target network을 사용하고\n",
    "# target network은 10번 에피소드에 한번씩만 policy network과 동기화 해주는 구조 (어려운 개념은 없다.)\n",
    "# 아래에서 배치사이즈 만큼 한번에 step돌리는거 같은데 그 구체적인 방법에 대해서 자세히 보지는 않았다. 나중에는 한번쯤은 봐야할듯\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        # 요개 내부 파라미터를 -1에서 1사이로 매번 조정해주는 기능같은데.. 왜 쓰는지 모르겠다.\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 아래가 메인루프라고 할 수 있겠다.\n",
    "# 이게 돌려보니까.. 입실론 그리디에서 랜덤으로 빠질때는 에피소드는 하나 소모되지만 실제 네트웍학습은 스킵되고\n",
    "# 리플레이 메모리 사이즈가 배치사이즈보다 작을때도 스킵되는등 초반에는 허수가 있다.\n",
    "# 실제로는 50은 택도없고 꽤나 큰수가 돼야 수렴되기 시작하는걸로 보인다.\n",
    "\n",
    "# 그리고 reward구조를 보면 step을 돌렸을때 살아남기만 하면 1.0을 주고\n",
    "# pole의 기울기가 너무 커지면 done이 True가 되면서 에피소드가 끝나는 구조이다.\n",
    "# 마지막에 done이 True일때 reward가 0으로 반영되는지는 확인안해봤는데 나중에 확인해보자. 0이라면 터미널 패널티가 있는 셈일듯 \n",
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
